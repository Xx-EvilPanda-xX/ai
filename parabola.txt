[1, 30, 1]

Layer 2:
    Node 1:
        Bias:
            2.7837093940252498
        Incoming weights:
            -0.13020951742267242
    Node 2:
        Bias:
            0.36843774080981573
        Incoming weights:
            -0.9825349449666767
    Node 3:
        Bias:
            1.0688786220289537
        Incoming weights:
            -0.06464187092378507
    Node 4:
        Bias:
            2.4701356224309645
        Incoming weights:
            0.29810652693000655
    Node 5:
        Bias:
            -4.016230398206066
        Incoming weights:
            0.41061169899409433
    Node 6:
        Bias:
            0.5814559161119828
        Incoming weights:
            -0.6953450566389595
    Node 7:
        Bias:
            -0.3105092423828563
        Incoming weights:
            0.4253201188329933
    Node 8:
        Bias:
            0.2698981087970673
        Incoming weights:
            0.5378164143151993
    Node 9:
        Bias:
            2.116161662357227
        Incoming weights:
            -0.2329842338165695
    Node 10:
        Bias:
            1.5596351405152142
        Incoming weights:
            -0.16856126658504364
    Node 11:
        Bias:
            -2.1259966767517473
        Incoming weights:
            0.245944630261196
    Node 12:
        Bias:
            -3.3255385302695144
        Incoming weights:
            -0.4310869706050969
    Node 13:
        Bias:
            -3.2860296072756774
        Incoming weights:
            0.25519215714655363
    Node 14:
        Bias:
            -2.0417575583261622
        Incoming weights:
            0.26739805033860364
    Node 15:
        Bias:
            2.5797744148783845
        Incoming weights:
            0.27311978509237766
    Node 16:
        Bias:
            -0.6612783735946041
        Incoming weights:
            -0.2204190507523488
    Node 17:
        Bias:
            -0.22088718480117736
        Incoming weights:
            0.28825265507004794
    Node 18:
        Bias:
            -2.907826085282782
        Incoming weights:
            -0.32898210051959514
    Node 19:
        Bias:
            0.5125394557452252
        Incoming weights:
            -0.3519961202745988
    Node 20:
        Bias:
            -2.8324670806933483
        Incoming weights:
            -0.344660847372794
    Node 21:
        Bias:
            -0.49082736836962904
        Incoming weights:
            0.8960817280679853
    Node 22:
        Bias:
            -0.7050274413652272
        Incoming weights:
            0.24601894752698553
    Node 23:
        Bias:
            -3.51102882676641
        Incoming weights:
            -0.334357640499005
    Node 24:
        Bias:
            -3.540513940381412
        Incoming weights:
            0.4144285111031054
    Node 25:
        Bias:
            1.0528719277568241
        Incoming weights:
            -0.29081801607297775
    Node 26:
        Bias:
            1.4435658704155514
        Incoming weights:
            0.13247498990796952
    Node 27:
        Bias:
            0.6369676762061252
        Incoming weights:
            0.27985623497844
    Node 28:
        Bias:
            -1.2301167274657234
        Incoming weights:
            -0.21601552932054238
    Node 29:
        Bias:
            -0.18025717637190983
        Incoming weights:
            -0.04986192888537305
    Node 30:
        Bias:
            -0.9167419114230791
        Incoming weights:
            -0.06248433563515136

Layer 3:
    Node 1:
        Bias:
            2.1799788277604946
        Incoming weights:
            -2.7842421401199053
            0.43691846605790957
            1.2647064703534117
            -2.2256031473753324
            5.610594036961803
            2.1402505148179443
            1.5123657402362674
            2.180648548174894
            -1.9606793707381485
            -1.1811432739874577
            2.8415643029291386
            4.148535631899229
            4.556558424672253
            2.6966543484961796
            -2.4422101543089614
            1.7943952781222556
            0.8508271833556155
            3.598075850852025
            1.8706988572359364
            3.3040980926058747
            0.6124015795035158
            1.096231085481031
            3.993330121527122
            4.593016737306578
            -0.7943452199435009
            -1.139451794773077
            -0.36836123674057675
            1.4840354263131712
            0.16348410466229046
            -0.4507332791303426

